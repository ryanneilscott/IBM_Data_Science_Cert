#We import our libraries

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
from sklearn.linear_model import LinearRegression
%matplotlib inline


# We retrieve our data from a source and use pandas to put it into a data frame

file_name='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/FinalModule_Coursera/data/kc_house_data_NaN.csv'
df=pd.read_csv(file_name)


# Use the head function to determine the first 5 rows of our data frame

df.head()


# Use the dtypes function to determine the types of data in our data frame

df.dtypes


# The describe function gives us a statistical summary of each column of our data frame

df.describe()


# We use the drop function to remove two identifier columns that we will not use in our calculations and verify they are removed

df.drop({"id", "Unnamed: 0"}, axis = 1, inplace=True)
df.describe()


# We use the null function to determine missing values in our data frame. We find that 13 missing values in bedroom and 10 missing in bathroom

print("number of NaN values for the column bedrooms :", df['bedrooms'].isnull().sum())
print("number of NaN values for the column bathrooms :", df['bathrooms'].isnull().sum())


# We replace the null values with the mean of the columns and use the replace function to replace these values. Then, we verify that there are no missing values

mean=df['bedrooms'].mean()
df['bedrooms'].replace(np.nan,mean, inplace=True)

mean=df['bathrooms'].mean()
df['bathrooms'].replace(np.nan,mean, inplace=True)

print("number of NaN values for the column bedrooms :", df['bedrooms'].isnull().sum())
print("number of NaN values for the column bathrooms :", df['bathrooms'].isnull().sum())


# We use the value count function to determine the number of unique floors and the to_frame function to view the results

df['floors'].value_counts().to_frame()


# We create a boxplot using the boxplot function from the seaborn library to determine the outliers of houses with and without a waterfront view

sns.boxplot(x="waterfront", y="price", data=df)


# We use the regplot function to determine if there is a positive or negative correlation with sqft_above and price. We determined that there is a positive correlation

sns.regplot(x="sqft_above", y="price", data=df)


# We fit a linear regression model using the feature 'long' and caculate the R^2. We get the value 0.00046769430149007363

X = df[['long']]
Y = df['price']
lm = LinearRegression()
lm.fit(X,Y)
lm.score(X, Y)


# We fit a linear regression model using the feature 'sqft_living' and caculate the R^2. We get the value 0.4928532179037931

X1 = df[['sqft_living']]
Y = df['price']
lm = LinearRegression()
lm.fit(X1,Y)
lm.score(X1, Y)


# We want to determine the R^2 of the listed features and the price. We calculated the value 0.6576569675583581

features = df[["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]]

lm.fit(features, df['price'])
lm.score(features, df['price'])


# We created a list of tuples. Then, we used the tuples to create a pipeline object to predict price and calculate the r^2, which was 0.8753236078885457

Input=[('scale',StandardScaler()),('polynomial', PolynomialFeatures(include_bias=False)),('model',LinearRegression())]

pipe=Pipeline(Input)

pipe.fit(features, df['price'])

ypipe=pipe.predict(features)
ypipe[0:10]

lm.fit(features, ypipe)

print('The R-square is: ', lm.score(features, ypipe))


# We imported the modules

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
print("done")


# We split the data into training and testing sets. There are 3242 testing samples and 18371 training samples

features =["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]    
X = df[features]
Y = df['price']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=1)


print("number of test samples:", x_test.shape[0])
print("number of training samples:",x_train.shape[0])


# We imported the Ridge Regression model from sklearn

from sklearn.linear_model import Ridge


# We fit the training data into a ridge regression object and find the R^2 of the testing data

RidgeModel=Ridge(alpha=1)

RidgeModel.fit(x_train, y_train)

RidgeModel.score(x_test, y_test)


